\documentclass[draft]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T2A]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[russian, english]{babel}
\usepackage[colorlinks,urlcolor=blue]{hyperref}

\def\RealSet{\mathbb{R}}
\newtheorem{Qual}{Quality}

\begin{document}

\section{Backpropagation explained}


So let's take a closer look on Neural Networks. We will start by reviewing Artificial Neural Networks (ANN), because they seem simple enough to start diving deep into this topic.


\subsection{Define Neural Networks}

Frequently NN is defined as a set of functions working on matrices and vectors. It's very bulky, but let's go thorough it. 

\paragraph{Functional notation}
I'll use definition from \cite{Ostwald:2021}

A multivariate vector-valued function
\[
f \colon \RealSet^{n_0} \rightarrow \RealSet^{n_k}, x\mapsto f(x)=:y
\]
is called a k-layered neural network, if $f$ is of the form
\begin{multline}
f \colon \RealSet^{n_0} \xrightarrow{\Phi_{W^1}^1} \RealSet^{n_1}
\xrightarrow{\Sigma^1} \RealSet^{n_1} \\
\RealSet^{n_1} \xrightarrow{\Phi_{W^2}^2} \RealSet^{n_2}
\xrightarrow{\Sigma^2} \RealSet^{n_3} \cdots \\
\cdots \RealSet^{n_{k-1}}
\xrightarrow{\Phi_{W^k}^k} \RealSet^{n_k}
\xrightarrow{\Sigma^k} \RealSet^{n_k}
\end{multline}
where for $l=1,\ldots,k$
\[
\Phi_{W^l}^l \colon \RealSet^{n_{l-1}} \rightarrow \RealSet^{n_l}, 
a^{l-1}\mapsto \Phi_{W^l}^l(a^{l-1}) :=W^l \cdot \begin{pmatrix} a^{l-1} \\ 1 \end{pmatrix} =: z^l
\]
are potential functions and
\[
\Sigma^l \colon \RealSet^{n_{l}} \rightarrow \RealSet^{n},
z^l \rightarrow \Sigma^l(z^l) =: a^l
\]
are component-wise activation functions. For $x\in \RealSet^{n_0}$, a $k$-layered neural network takes on the value
\[
f(x) := \Sigma^k( \Phi_{W^k}^k( \Sigma^{k-1}( \Phi_{W^{k-1}}^{k-1}(\cdots \Sigma^1( \Phi_{W^1}^1(x) ) \cdots) ) ) ) \in \RealSet^{n_k}
\]

Then the weight matrix-variate neural network function $f_x$ of $f$ is deifned as the function
\[
f_x \colon \RealSet^{n_1 \times (n_0 + 1)} \times \cdots \times \RealSet^{n_k \times (n_{k-1} + 1)} \rightarrow \RealSet^{n_k}
\]

\begin{multline}
(W^1,\dots,W^k)\mapsto f_x(W^1,\dots,W^k) := \\
\Sigma^k( \Phi^k( W^k, \underbrace{\Sigma^{k-1}( \Phi^{k-1}( W^{k-1}, \cdots (W^2, \overbrace{\Sigma^1( \Phi^1(W^1, x))}^{a^1}) \cdots) ) }_{a^{k-1}} ))
\end{multline}

\paragraph{Matrix notation}
I really like the following elegant definition through tuples from \cite{Karner:2022}. Let $d, L \in \mathbb{N}$. A neural network (NN) with input dimension $d$ and $L$ layers is
a sequence of matrix-vector tuples
\[
\Phi := ((A_1, b_1), (A_2, b_2), \ldots , (A_L, b_L)),
\]

where $N_0 := d$ and $N_1, \ldots , N_L \in \mathbb{N}$, and where $A_j \in \RealSet^{N_{j}\times N_{j-1}}$ and $b_j \in \RealSet^{N_j}$ for $j = 1, \ldots, L$. The number $N_L$ is referred to as the output dimension.


\subsection{Forward pass}

\paragraph{Functional notation}
The last column of $W^l$ encodes the biases of the neurons in layer $l$. The activation of neuron $i$ in layer $l$ for $i = 1,\dots,n_l$ and $l = 1,\dots,k$ is given by
\[
a^l_i = \sigma \left( \sum_{j=1}^{n_{l-1}} w_{ij}^l a_j^{l-1} + w_{i,n_{l-1}+1} \right) \in \RealSet^{n_l}
\]


\paragraph{Matrix notation}
For a NN $\Phi$ and a domain $\Omega\subset\RealSet^d$, we define the associated realisation of the NN $\Phi$ as
\[
R(\Phi) \colon \Omega \rightarrow \RealSet^{N_L} \colon x \mapsto x^{(L)}:=R(\Phi)(x),
\]
The output $x^{(L)}\in \RealSet^{N_L}$ results from
\begin{align}
x^{(0)} &:= x,\\
x^{(j)} &:= \varrho(A_j x^{(j-1)} + b_j) \text{ for $j=1,\dots,L-1$}, \label{karner-forward-j}\\
x^{(L)} &:= A_L x^{(L-1)} + b_L.
\end{align}
Here $\varrho(x) := \max \{0,x\}$, for $x\in\RealSet$ is understood to act component-wise on vector-valued inputs, i.e., for $y=(y^1,\dots,y^m)\in\RealSet^m$, $\varrho(y) := (\varrho(y^1),\dots,\varrho(y^m))$

I don't understand why authors decided to take this route and go against more or less canonical notation. We need to swap indices up and down and rename some letters this way:
\begin{align}
x^{(j)} &\mapsto a^j \\
A_j &\mapsto W^j \\
b_j &\mapsto b^j \\
\varrho(.) &\mapsto f(.) \\
\end{align}

As a result we can rewrite~\eqref{karner-forward-j} and get very familiar two step form
\begin{align}
z^l &= W^l \cdot a^{l-1} + b^l \\
a^l &= f(z^l)
\end{align}


\subsection{Gradient descent}

\paragraph{Functional notation}
A multi-matrix-variate real-valued function of the form
\[
c_{xy} \colon \RealSet^{n_1 \times (n_0 + 1)} \times \cdots \times \RealSet^{n_k \times (n_{k-1} + 1)} \rightarrow \RealSet,
\]
\[
(W^1,\dots,W^k) \mapsto c_{xy}(W^1,\dots,W^k) := c_y(f_x(W^1,\dots,W^k))
\]

Select $\mathcal{W}^{(0)}$ and $\alpha > 0$ appropriately. For $j = 1,2,\dots,$(until convergence), set
\[
\mathcal{W}^{(j)} := \mathcal{W}^{(j-1)} - \frac{\alpha}{n} \sum_{i=1}^n \nabla c_{x^{(i)}y^{(i)}}(W^1,\dots,W^k),
\]
where,
\[
\nabla c_{x^{(i)}y^{(i)}}(W^1,\dots,W^k) = \Bigl(\nabla_{W^l} c_{x^{(i)}y^{(i)}}(W^1,\dots,W^k) \Bigr)_{1\leqslant l \leqslant k}
\]
denotes the gradient of the $i$-th training exemplar-specific cost function for $i = 1,\dots,n$

Each of these gradients for all $i = 1,\dots,n_l$, $j = 1,\dots,n_{l-1} + 1$, and $l = 1,\dots,k$ comprises the partial derivatives
\[
\nabla_{W^l} c_{xy}(W^1,\dots,W^k) = \Biggl( \frac{\partial}{\partial w_{ij}^l} c_{xy}(W^1,\dots,W^k) \Biggr)_{ij}
\]


\paragraph{Matrix notation}
Again repeat definitions from \cite{Karner:2022}.

Loss function $\ell \colon \RealSet^q \times  \RealSet^q \rightarrow  \RealSet^{+}$, for $q \in \mathbb{N}$, which could be the square loss $\ell(y,y^\prime)=\| y - y^\prime \|^2$. Then for $M \in \mathbb{N}$, we define $\mathcal{R} \colon (\RealSet^q)^M \times (\RealSet^q)^M \rightarrow \RealSet$ by
\[
\hat{\mathcal{R}} (Y,\hat Y) = \frac1{M} \sum_{i=1}^M \ell(Y_i, \hat Y_i)
\]

for all $Y=(Y_i)_{i=1}^M \subset \RealSet^q$, $\hat Y = (\hat Y_i)_{i=1}^M \subset \RealSet^q$

Let $j \in {1,\ldots,L}$. The exact gradient descent update of the biases in the $j$-th layer is given by $u_j^b$, which is defined as

\begin{align}
u_j^b &:= \frac1{M} \sum_{i=1}^M u_{j,i}^b, \text{where} \\
u_{j,i}^b &:= I_j(x_i)A_{j+1}^T I_{j+1}(x_i) A_{j+2}^T \cdots I_{L-1}(x_i)A_L^T \ell^\prime(y_i, R(\Phi_j)(x_i))
\end{align}
for $i\in {1,\ldots,M}$, where $I_j(x)\in{0,1}^{N_j}$ with $(I_j(x))_k = 1$ if and only if $R(\Phi_j)(x)\geqslant0$


\section{The toy task}

\paragraph{The goal}
What I want to formulate is the system that can learn almost instantaneously, adapt to the changing world, and follow its core needs. Also create completely new, not in any way prior programmed goals, in which the system sees a better strategy. Let’s formulate environment conditions, agent goals, and preferable strategies that will be used in our new model. We will aim for a simple example and setup that will help better highlight the problem and test our methods.

We start with a traditional setting. Usually it includes an agent in 2D space environment that can freely move around in the environment, search for a reward, and receive a feedback from the environment which can affect the agent in negative or positive manner. The environment can be a pretty simple grid or it can have obstacles, thus adding complexity to navigation rules, or the agent might be forced to look for the shortest route.

\paragraph{ANN}
Although somewhere in the first paragraphs researchers promise to create very smart, self-contained algorithms making substantive decisions, but in the end they start with very limiting environment. Our starting point will be an ANN as the simplest possible model.

In fact, it crudely simplifies many of the biological aspects of the neuron. If we make a biological version of ANN model, and put it into fMRI scanner, and scan its activity during its work, then we will see that it activates precisely one layer at a time.

\paragraph{Gradient descent}
ANN is a classifier and an approximator. 
For the learning it requires a training set with labeled outcomes. 
ANN is trained with the technique called Gradient Descent (GD). There are other approaches, but essentially they all are trying to solve an optimization problem.
GD needs to correct system parameters step by step, and in the end the system will produce output closer to the required one.
This means that even though the agent can obtain new inputs from the environment at any moment, but ANN cannot label output values during training by its own. However after the training process has been performed, it can define labels by approximating the input, but its choice is limited by the data it was trained on.

ANN goes through the training set over and over, updating weights and reducing value of the cost function (mean square function, for example). At some point the learning process stops and never resumes after. Hence, we introduce the quality 

\begin{Qual}\label{q1}
the system must start taking actions after the first “training input” is received
\end{Qual}

\paragraph{Unsupervised learning}
The teacher requirement should be loosen. 

It can be achieved by considering another algorithms that learn without a teacher. This class is called unsupervised learning. Very popular probabilistic methods like principal component analysis (PCA) and k-means clustering, they also try to mimic the training data, but do not answer to the main question: how to live with less cost on consumed energy and more reward

In our framework we should find a way to punish probabilistic approaches. Surely it all boils down to activation threshold and impulse routes between neurons.
Neuron is a small cell that transmits signals with respect to some external and internal conditions (chemistry saturation level, time sequence of spikes). But if such algorithm only discovers a distribution in training data, then what it can learn from a coin toss. Only that coin is fair or not. But nothing about what is a coin or speculate about how many times in a row it can land on head. Quite the opposite, algorithms can restore data out of very big noise, which is impressive, but does not lead us to the point.


\paragraph{Reinforcment learning}
In reinforcement learning we have an environment, states, actions, probabilities to take an action and advance from one state to another, reward function for such transition. The agent looks for policies: what actions to take when it finds itself in a specific state.

But

\begin{itemize}
\item  not on every step you should receive a reward. maybe a long chain of struggle leads to something incredibly good. Can be solved by a notion of regret
\item some policies can be “prewired” or form of bad habits - hard to change, but repeatable work can do it.
\item also uncontrolled biological condition can affect policies temporarily (low battery for robot, depression - for humans). The agent should understand that the state is not permanent, policy shouldn’t be adjusted, but another alternative policies (and also temporal) can be developed
\item maybe not important, but really artificial thing that really itches: if best policy is found, so the object will continue doing the same sequence of actions over and over without any alteration. It will not get bored doing only perfect actions.
\item technical downside is learning by one “training step” at a time, because of the environment response
\end{itemize}

The only way to know the reaction from the environment is to take an action. Which satisfies our quality of “learning by doing“ \ref{q1}

\paragraph{A sequence of actions}
Training data has a defined set of possible outcomes. In terms of Reinforcement Learning (RL) we require the system to follow strategies and increase total reward value of the system over time.
Learning process can be just a random walk or with the help of genetic algorithm, when output of the reward function must be maximized.
Although the training data could have negatively labeled result, but ANN in its original structure cannot use negative outcomes. So we cannot define correct and incorrect labels for every input, which means that they all must lead to a positive result on every action. This leads us to several conclusions. 

\begin{Qual}\label{q2}
  predicting negative result should be taken into account
\end{Qual}

\paragraph{Memory}

The system should have memory in order to store appropriate responses to input signals that lead to a reward increase (in terms of RL).

\begin{Qual}\label{q3}
  actions are important in sequence
\end{Qual}

A sequence of actions might be important to find the best strategy, thus RNN can be a good fit because they can save sequences in its memory. Output of the network can be connected to motor activation. This is how RNN is designed. Also it means that “the decision” is made somewhere in the hidden layers. 



\paragraph{Conclusions}
ANN alone doesn’t extract logic about inputs on how one is different from another. There is no hidden semantics, it doesn’t create relations or conclusions based on data. As a reminder ANN is limited to classification and approximation problems.



\paragraph{Connections}
What if by putting all responsibility on connections is wrong. Dendrites and axons only transmit the signal. And when the signal is emitted, nothing can stop it (only degenerative disease can). It can be “ignored” be the receiving neuron.

But here is the difference: decision is made not by connections, but by neuron itself. And that decision is made not because of weights and the signal together not reaching the threshold, but instead in dynamics it changes its internal state based on a sequence of inputs. It sounds strange, but I think it only makes sense when we consider a group of neurons. Let’s assume that neuron is not functional, we always must consider the work in conjunction of several units. 

\medskip\hrule\medskip

It must be a subject of transfer learning to adjust known strategies to changing environment. Here I have seen few interesting approaches in the MDP framework. In short, in MDP we have actions, policies and states. Usually only policies are dynamic, but states and actions are pre-defined before experiments. Some researches make states dynamic and the agent can add more states and remove bad ones. For some reason it’s not fully dynamical. One system did require the first state to be defined. Another more or less works on state subsets (states are defined but not all of them available).


What about actions? That component should be considered dynamic too.

Can MDP be defined purely in ANN terms (I don’t mean gradient descent or back propagation)? Why? Then it can be compared with RNN not performance vise, but structure vice. Same as the opposite is applicable: sequential memory can be added to MDP.


ANN should have notion of time. RNN can do it

\url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks}

How to encode time in ANN. With the help of another ANN that will nonlinearly transform idle delay time into encoded input. Almost like one-hot encoding, but they call it soft.

\url{https://towardsdatascience.com/how-to-encode-time-property-in-recurrent-neutral-networks-friday-experiment-c14c39ba9755}

\url{https://github.com/crazyleg/time-dependant-rnn-embeddings-keras}

\url{https://arxiv.org/pdf/1708.00065.pdf}

\url{https://github.com/brendenlake/omniglot} - omniglot dataset

How to represent time in hidden neurons. Let’s assume that we have neurons “second”, “minute”, “hour”, “day”. How to start counting in ANN? How to write a calculator with ANN?

All new qualities

- **must start taking actions after the first “training input” is received**
- **should have memory in order to store appropriate responses to input signals that lead to a reward increase**


\bibliographystyle{apalike}
\bibliography{bibfile}

\end{document}